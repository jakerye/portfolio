# Levereage IoT Device Backend
*Describe a project that you worked on that you are proud of. It's best if you can describe it from start to finish including the motivation and technical details. If you are describing a team effort, please be specific about your personal contribution.*

## Overview
When I was working at Leverage, I built the entire IoT device backend for an asset tracking product from a 0-1 state. This scaled to >250K connected devices that checked in every 15-60 minutes. I did have another engineer I collaborated with but his attention was split between multiple projects and focused more on the devops side of things. 

## Context
My company was historically a consulting business but became interested in launching their own IoT asset tracking product when I joined. We partnered with a large hardware company who built the devices and wrote the firmware for them. My company had a generic asset tracking based UI that we would white-label for our clients so there was a need to build the IoT device / application backend to connect these two pieces.

I collaborated with our hardware partner to build the product MVP, launch it, scale it, and then expand our product offering.

## Application
The product was built in a generic way so that we could use it for multiple asset-tracking applications, but our first application was offering a device to car dealerships to help them with their lot management (figure out where their cars were), but they could also sell this device to car buyers as a theft recovery solution. This business model made it easy to get in the door with car dealerships as traditional lot management solutions were a cost to them, however, this model made it a new revenue stream for them. As the devices had cellular, wifi, and bluetooth comms, they could be used for both outdoor (GPS) and indoor (nearby beacons) locationing.

## Scope
The device backend was in charge of managing the fleet of devices (firmware over-the-air upgrades, device configuration), processing their telemetry data (location, nearby bluetooth devices, battery level, operation state), and sending them commands to change their state (e.g. upgrade the main controller’s firmware to new version, enter a more power intensive operation mode if theft was suspected).

The backend was split into two servers – one that was specific to the operation of our partner’s device set, and another one that was a generic firmware over-the-air updates manager that could be re-used in the future for different devices. Our partner’s devices were built to be generic as well so the same device server was used to interface with both the locators and the beacons (and in theory future devices).

## Challenges
There were plenty of early stage challenges that were not foreseen in the beginning but we were able to overcome. We ended up doing a lot more firmware upgrades than we projected as early stage firmware is hard to get right the first time, and many bugs only uncover themselves after the device burns in for a few months. One notable bug, the “time bomb” bug as we called it, caused the devices to brick due to a timer overflowing the max int size and crashing the firmware (also disabling the watchdogs). The root cause of this bug was not immediately obvious as many of our devices would just suddenly go offline and never come back on. To help debug this, I analyzed our fleet of devices to find any pattern between the devices suddenly going offline. The pattern that helped us find the root cause was noticing all devices that were going offline were actively reporting to the backend for ~50 days, then went dark. This made us suspicious of something related to a time-based process failing not-so gracefully. After brainstorming with my team, one of the seasoned engineers put together the significance of 50 days: 2^32 milliseconds = 49.7 days. After sharing our findings with our partner, they confirmed it was a timer overflow issue, scrambled to make a new firmware release, identify which devices needed to receive the new firmware the soonest, and then we rolled it out.

To make matters more complex, this was our first large-scale firmware rollout, and when we pushed the new firmware to one of our first dealerships, many of the devices did not upgrade. We had previously performed a test on ~100 internal devices and all devices upgraded as expected. However, now that we were trying to upgrade ~10K devices, something was breaking down. This was a stressful moment as upgrading the fleet was my responsibility and it was not working – and we only had a few weeks left before all our devices would get bricked. I combed through all my code, firmware configurations, and logs – hunting for obscure edge cases that could explain this, but there was no obvious bug. After retrieving one of the devices that failed the firmware upgrade and chatting with our MVNO, we found another problem. Since so many devices were trying to pull the firmware files at the same time, the cellular band our devices were provisioned to collectively exceeded the bandwidth and the devices experienced firmware file packet-loss. As the devices did not have any retry mechanism for packet-loss, the upgrades failed. For a near term solution, I added a throttling mechanism to limit the number of active firmware upgrades per cell tower while the firmware team improved their packet loss handling. Between these two solutions, we were able to upgrade all the devices and avoided bricking our entire fleet.

Once we got over these initial challenges, the product ran smoothly and rollouts continued to scale. Our teams expanded the product offering and got into a new key tracking product – adding an additional revenue stream for our companies.

# Conclusion
After the product had stabilized and the project was entering more of a maintenance mode, I got recruited to another company, transitioned the project to a new engineer and consulted with him occasionally to ensure things stayed running smoothly. To this day, the project is still growing and the fleet remains unbricked.
